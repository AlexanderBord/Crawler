<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Readme</title>
</head>
<body>

<h1>Internet Course B - Exercise 3</h1>
<h2>Author</h2>
<ul>
    <li>Alexander Bord</li>
    <li>ID 319263836</li>
    <li>hlexandorbo@hac.ac.il</li>
</ul>
<hr>
<h3> Api <a href="doc/exercises/ex3_bord_alexander/package-summary.html"> click here </a></h3>
<hr>
<h2>Execution</h2>
<pre>
Threads exercise -

    In this exercise we built a site that allows crawling URL and returns the number
    of images found recursively on the page.
    The recursion process is based on links: whenever a link is found,
    the crawler runs recursively on the URLs.
    The scanning process is performed using a private process produced for each user.

    A crawler is a program that navigates the Internet and finds pages for analysis.
    The scanner starts with a staged URL (seed / root) and analyzes recursive URLs linked from this page.
    The scanner usually has a maximum depth parameter for limiting the recursion depth.
    The crawler also maintains a list of visited URLs so as not to repeat the recursion process endlessly.

</pre>

<h2>Assumptions</h2>
<p>Jsoup library, org.glassfish:javax.json:1.1.14</p>

<address>Submitted 12 may 2021 </address>
</body>
</html>